#!/usr/bin/env python3  

import requests  
import re  
from urllib.parse import urljoin, urlparse  
import concurrent.futures  
import colorama  
import argparse  
import os  
import json  
import networkx as nx  
import matplotlib.pyplot as plt  
from bs4 import BeautifulSoup  
import logging  
from urllib3.exceptions import InsecureRequestWarning  
import time  

class AdvancedURLSpider:  
    def __init__(self, base_url, max_depth=3, max_workers=20):  
        # Configuration  
        self.base_url = base_url  
        self.max_depth = max_depth  
        self.max_workers = max_workers  
        
        # Data structures  
        self.visited_urls = set()  
        self.discovered_urls = {}  
        self.url_graph = nx.DiGraph()  
        
        # Logging setup  
        self.setup_logging()  
        
        # Suppress SSL warnings  
        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)  
        
        # Colorama initialization  
        colorama.init(autoreset=True)  

    def setup_logging(self):  
        logging.basicConfig(  
            level=logging.INFO,  
            format='%(asctime)s - %(levelname)s: %(message)s',  
            handlers=[  
                logging.FileHandler('spider_log.txt'),  
                logging.StreamHandler()  
            ]  
        )  
        self.logger = logging.getLogger(__name__)  

    def get_session(self):  
        session = requests.Session()  
        session.headers.update({  
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',  
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8'  
        })  
        session.verify = False  # Disable SSL verification  
        return session  

    def extract_urls(self, html_content, base_url):  
        soup = BeautifulSoup(html_content, 'html.parser')  
        urls = set()  

        # Extract URLs from different sources  
        for tag in soup.find_all(['a', 'link', 'script', 'img'], href=True):  
            url = tag.get('href')  
            full_url = urljoin(base_url, url)  
            urls.add(full_url)  

        # Additional URL extraction using regex  
        additional_urls = re.findall(r'https?://[^\s<>"]+|/[^\s<>"]+', html_content)  
        urls.update(additional_urls)  

        return urls  

    def is_valid_url(self, url):  
        parsed = urlparse(url)  
        return (  
            parsed.scheme in ['http', 'https'] and   
            parsed.netloc == urlparse(self.base_url).netloc  
        )  

    def spider_url(self, url, depth=0):  
        if (url in self.visited_urls or   
            depth > self.max_depth or   
            not self.is_valid_url(url)):  
            return set()  

        try:  
            session = self.get_session()  
            response = session.get(url, timeout=5)  
            
            if response.status_code == 200:  
                self.visited_urls.add(url)  
                self.discovered_urls[url] = {  
                    'status_code': response.status_code,  
                    'content_type': response.headers.get('Content-Type'),  
                    'depth': depth  
                }  

                # Log discovered URL  
                self.logger.info(f"Discovered: {url} (Depth: {depth})")  

                # Extract URLs  
                discovered = self.extract_urls(response.text, url)  
                valid_urls = {u for u in discovered if self.is_valid_url(u)}  

                # Build URL graph  
                for discovered_url in valid_urls:  
                    self.url_graph.add_edge(url, discovered_url)  

                return valid_urls  
            
        except Exception as e:  
            self.logger.warning(f"Error spidering {url}: {e}")  
        
        return set()  

    def parallel_spider(self):  
        urls_to_spider = {self.base_url}  
        
        for depth in range(self.max_depth + 1):  
            new_urls = set()  
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:  
                future_to_url = {  
                    executor.submit(self.spider_url, url, depth): url   
                    for url in urls_to_spider  
                }  
                
                for future in concurrent.futures.as_completed(future_to_url):  
                    try:  
                        result = future.result()  
                        new_urls.update(result)  
                    except Exception as e:  
                        self.logger.error(f"Unexpected error: {e}")  
            
            urls_to_spider = new_urls - self.visited_urls  

            if not urls_to_spider:  
                break  

    def visualize_url_graph(self):  
        plt.figure(figsize=(20, 20))  
        pos = nx.spring_layout(self.url_graph)  
        nx.draw(self.url_graph, pos, with_labels=False, node_color='lightblue',   
                node_size=50, edge_color='gray', alpha=0.6)  
        plt.title("URL Discovery Graph")  
        plt.savefig('url_graph.png')  
        plt.close()  

    def save_results(self):  
        # Save discovered URLs  
        with open('discovered_urls.json', 'w') as f:  
            json.dump(self.discovered_urls, f, indent=4)  

        # Save URL graph  
        nx.write_gexf(self.url_graph, 'url_graph.gexf')  

def main():  
    parser = argparse.ArgumentParser(description="Advanced URL Spider")  
    parser.add_argument('-u', '--url', required=True, help='Base URL to spider')  
    parser.add_argument('-d', '--depth', type=int, default=3, help='Maximum crawl depth')  
    parser.add_argument('-w', '--workers', type=int, default=20, help='Number of concurrent workers')  
    
    args = parser.parse_args()  

    spider = AdvancedURLSpider(  
        base_url=args.url,   
        max_depth=args.depth,   
        max_workers=args.workers  
    )  

    print(colorama.Fore.GREEN + f"[*] Starting URL spidering for {args.url}")  
    start_time = time.time()  
    spider.parallel_spider()  
    
    print(colorama.Fore.CYAN + "[*] Saving results...")  
    spider.save_results()  
    
    print(colorama.Fore.YELLOW + "[*] Generating URL graph visualization...")  
    spider.visualize_url_graph()  

    end_time = time.time()  
    elapsed_time = end_time - start_time  
    print(colorama.Fore.GREEN + f"[+] Spidering complete! Elapsed time: {elapsed_time:.2f} seconds")  

if __name__ == '__main__':  
    main()
